{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05fe228b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaMulticore\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import wordnet\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7cd83e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "def text_from_pdf(folder):\n",
    "    docs=[]\n",
    "    files=os.listdir(folder)\n",
    "    for i in range(len(files)):\n",
    "        filename= os.path.join(folder, files[i])\n",
    "        with fitz.open(filename) as doc:\n",
    "            for page in doc:\n",
    "                text = page.get_text() \n",
    "                text = text.lower().replace(\"\\n\",\" \").replace(\"  \", \" \")\n",
    "                docs.append(text)\n",
    "    return docs\n",
    "\n",
    "docs=text_from_pdf(\"/Users/SB6282engie.com/Documents/Policy documents/Bristol\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "027ec56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(doc_set):\n",
    "    \"\"\"\n",
    "    Input  : docuemnt list\n",
    "    Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n",
    "    Output : preprocessed text\n",
    "    \"\"\"\n",
    "    # initialize regex tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # create English stop words list\n",
    "    en_stop = list(set(stopwords.words('english')))+[\"bristol\", \"city\",\"policy\",\"strategy\",\"strategies\"]\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    lemmatizer = wordnet.WordNetLemmatizer()\n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "    # loop through document list\n",
    "    for i in doc_set:\n",
    "        # clean and tokenize document string\n",
    "        tokens = tokenizer.tokenize(i)\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop and not i.isdigit() and len(i)>2]\n",
    "        # stem tokens\n",
    "        stemmed_tokens = [lemmatizer.lemmatize(i) for i in stopped_tokens]\n",
    "        # add tokens to list\n",
    "        texts.append(stemmed_tokens)\n",
    "    return texts\n",
    "clean_docs= preprocess_data(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "98cf2c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(doc_clean):\n",
    "    \"\"\"\n",
    "    Input  : clean document\n",
    "    Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\n",
    "    Output : term dictionary and Document Term Matrix\n",
    "    \"\"\"\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    # generate LDA model\n",
    "    return dictionary,doc_term_matrix\n",
    "dic,matrix = prepare_corpus(clean_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "27f0aed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start compute\n",
      "0  /  29 last step took  1.6689300537109375e-06 s\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LsiModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-ae7e7940debf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mplot_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_docs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-49-ae7e7940debf>\u001b[0m in \u001b[0;36mplot_graph\u001b[0;34m(doc_clean, start, stop, step)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_clean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdoc_term_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepare_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     model_list, coherence_values = compute_coherence_values(dic, matrix,doc_clean,\n\u001b[0m\u001b[1;32m     30\u001b[0m                                                             stop, start, step)\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Show graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-ae7e7940debf>\u001b[0m in \u001b[0;36mcompute_coherence_values\u001b[0;34m(dictionary, doc_term_matrix, doc_clean, stop, start, step)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0ms1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# generate LSA model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLsiModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_term_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mmodel_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mcoherencemodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoherenceModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoc_clean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoherence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c_v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LsiModel' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Input   : dictionary : Gensim dictionary\n",
    "              corpus : Gensim corpus\n",
    "              texts : List of input texts\n",
    "              stop : Max num of topics\n",
    "    purpose : Compute c_v coherence for various number of topics\n",
    "    Output  : model_list : List of LSA topic models\n",
    "              coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    print(\"Start compute\")\n",
    "    model_list = []\n",
    "    s1=time.time()\n",
    "    for num_topics in range(start, stop, step):\n",
    "        s=time.time()\n",
    "        print(int((num_topics-start)/step), \" / \", len(range(start, stop, step)),\"last step took \", s-s1, \"s\")\n",
    "        s1=s\n",
    "        # generate LSA model\n",
    "        model = LsiModel(doc_term_matrix, num_topics=num_topics, id2word = dictionary)  # train model\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values\n",
    "\n",
    "def plot_graph(doc_clean,start, stop, step):\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    model_list, coherence_values = compute_coherence_values(dic, matrix,doc_clean,\n",
    "                                                            stop, start, step)\n",
    "    # Show graph\n",
    "    print('highest coherence for : ', np.argmax(coherence_values))\n",
    "    x = range(start, stop, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.show()\n",
    "start,stop,step=1,30,1\n",
    "plot_graph(clean_docs,start,stop,step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ae77df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gensim_lda_model(doc_clean,number_of_topics,words):\n",
    "    \"\"\"\n",
    "    Input  : clean document, number of topics and number of words associated with each topic\n",
    "    Purpose: create LSA model using gensim\n",
    "    Output : return LSA model\n",
    "    \"\"\"\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    # generate LSA model\n",
    "    lsamodel = LdaMulticore(doc_term_matrix, num_topics=number_of_topics, id2word = words)  # train model\n",
    "    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=7))\n",
    "    return lsamodel\n",
    "number_of_topics=16\n",
    "lsamodel = create_gensim_lda_model(clean_docs, number_of_topics, dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eae86322",
   "metadata": {},
   "outputs": [],
   "source": [
    "SDG=[\"Poverty\", \"hunger\", \"health\",\"education\",\"gender equality\",\"water\",\"energy\",\"economic\",\"equality\", \"sustainable\",\"production\",\"climate\",\"life\", \"peace\"]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "15c0a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sense2vec import Sense2Vec \n",
    "import numpy as np\n",
    "s2v = Sense2Vec().from_disk(\"/Users/SB6282engie.com/Downloads/s2v_old\")\n",
    "def extract_words_topics(lsamodel, number_of_topics):\n",
    "    L=[]\n",
    "    for i in range(number_of_topics):\n",
    "        if len(L) == 0 or len(L[-1])>0:\n",
    "            L.append([])\n",
    "        topic=lsamodel.show_topic(i)\n",
    "        for j in range(len(topic)):\n",
    "            if topic[j][1]>0:\n",
    "                L[-1].append(s2v.get_best_sense(topic[j][0]))\n",
    "    return L\n",
    "word_topics=extract_words_topics(lsamodel, number_of_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "75aa5afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "similarity_matrix=np.zeros((len(SDG), len(word_topics)))\n",
    "for i in range(len(SDG)):\n",
    "    SDGlist = SDG[i].split(\" \")\n",
    "    for j in range(len(SDGlist)):\n",
    "        SDGlist[j] = s2v.get_best_sense(SDGlist[j])\n",
    "    for j in range(len(word_topics)):\n",
    "        similarity_matrix[i,j] = s2v.similarity(SDGlist, word_topics[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1dca0843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 sustainable ['strategy|NOUN', 'space|NOUN', 'people|NOUN', 'local|ADJ', 'plan|VERB', 'council|NOUN', 'community|NOUN', 'development|NOUN', 'green|ADJ', 'transport|NOUN']\n",
      "1 sustainable ['development|NOUN', 'plan|VERB', 'local|ADJ', 'new|ADJ', 'strategy|NOUN', 'policy|NOUN', 'transport|NOUN', 'business|NOUN', 'climate|NOUN', 'waste|VERB']\n",
      "2 sustainable ['development|NOUN', 'people|NOUN', 'area|NOUN', 'transport|NOUN', 'local|ADJ', 'space|NOUN', 'community|NOUN', 'plan|VERB', 'new|ADJ', 'need|VERB']\n",
      "3 sustainable ['people|NOUN', 'need|VERB', 'strategy|NOUN', 'area|NOUN', 'local|ADJ', 'community|NOUN', 'service|NOUN', 'new|ADJ', 'space|NOUN', 'development|NOUN']\n",
      "4 energy ['waste|VERB', 'people|NOUN', 'strategy|NOUN', 'need|VERB', 'local|ADJ', 'service|NOUN', 'provision|NOUN', 'space|NOUN', 'centre|NOUN', 'green|ADJ']\n",
      "5 climate ['strategy|NOUN', 'development|NOUN', 'local|ADJ', 'plan|VERB', 'change|VERB', 'climate|NOUN', 'centre|NOUN', 'need|VERB', 'area|NOUN', 'housing|NOUN']\n",
      "6 sustainable ['development|NOUN', 'strategy|NOUN', 'area|NOUN', 'space|NOUN', 'local|ADJ', 'green|ADJ', 'transport|NOUN', 'park|NOUN', 'plan|VERB', 'people|NOUN']\n",
      "7 education ['strategy|NOUN', 'service|NOUN', 'development|NOUN', 'local|ADJ', 'space|NOUN', 'climate|NOUN', 'need|VERB', 'housing|NOUN', 'people|NOUN', 'public|ADJ']\n",
      "8 sustainable ['strategy|NOUN', 'transport|NOUN', 'new|ADJ', 'area|NOUN', 'development|NOUN', 'space|NOUN', 'green|ADJ', 'local|ADJ', 'people|NOUN', 'council|NOUN']\n",
      "9 sustainable ['development|NOUN', 'plan|VERB', 'strategy|NOUN', 'area|NOUN', 'space|NOUN', 'need|VERB', 'park|NOUN', 'new|ADJ', 'local|ADJ', 'site|NOUN']\n",
      "10 life ['people|NOUN', 'strategy|NOUN', 'need|VERB', 'year|NOUN', 'health|NOUN', 'work|VERB', 'green|ADJ', 'community|NOUN', 'space|NOUN', 'road|NOUN']\n",
      "11 sustainable ['plan|VERB', 'park|NOUN', 'strategy|NOUN', 'council|NOUN', 'local|ADJ', 'need|VERB', 'people|NOUN', 'work|VERB', 'support|VERB', 'development|NOUN']\n",
      "12 life ['people|NOUN', 'space|NOUN', 'park|NOUN', 'community|NOUN', 'child|NOUN', 'transport|NOUN', 'new|ADJ', 'investment|NOUN', 'year|NOUN', 'local|ADJ']\n",
      "13 sustainable ['development|NOUN', 'area|NOUN', 'new|ADJ', 'local|ADJ', 'plan|VERB', 'strategy|NOUN', 'community|NOUN', 'people|NOUN', 'draft|NOUN', 'space|NOUN']\n",
      "14 life ['people|NOUN', 'road|NOUN', 'park|NOUN', 'local|ADJ', 'plan|VERB', 'strategy|NOUN', 'council|NOUN', 'new|ADJ', 'centre|NOUN', 'community|NOUN']\n",
      "15 economic ['new|ADJ', 'community|NOUN', 'development|NOUN', 'local|ADJ', 'cultural|ADJ', 'plan|VERB', 'strategy|NOUN', 'culture|NOUN', 'centre|NOUN', 'work|VERB']\n"
     ]
    }
   ],
   "source": [
    "topics_names = [SDG[i] for i in np.argmax(similarity_matrix, axis=0)]\n",
    "for i in range(len(topics_names)):\n",
    "    print(i, topics_names[i], word_topics[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "69653745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9,  9,  9,  9,  6, 11,  9,  3,  9,  9, 12,  9, 12,  9, 12,  7])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(similarity_matrix[:,:], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ee120d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15,), (15, 16))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(SDG),np.shape(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff94ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in s2v:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46bbf18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
